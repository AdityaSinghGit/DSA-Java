
⚪ Here's a table sorting various sorting algorithms from best to worst based on their average-case time complexities:

Algorithm	                    Average-case Complexity
-------------------------------------------------------
Merge Sort	                    O(n log n)
Quick Sort	                    O(n log n)
Heap Sort	                    O(n log n)
Tim Sort	                    O(n log n)
Smooth Sort	                    O(n log n)
Count Sort	                    O(n + k)
Radix Sort	                    O(nk)
Cycle Sort	                    O(n^2)
Shell Sort	                    O(n log^2 n)
Comb Sort	                    O(n log n)
Introsort	                    O(n log n)
Cocktail Sort	                O(n^2)
Selection Sort	                O(n^2)
Insertion Sort	                O(n^2)
Bubble Sort	                    O(n^2)


⚪ COUNT SORT and RADIX SORT have linear time complexities, but they have certain limitations 
    that make them less versatile than Merge Sort and Quick Sort in practice:

1- Space Complexity: Count Sort and Radix Sort often require additional space to store the counts or buckets, 
                  which can be impractical for large datasets or when memory is limited. Merge Sort and Quick Sort, 
                  although they have higher time complexities, usually require less additional space.

2- Data Type Constraints: Count Sort and Radix Sort are specifically designed for sorting integers or objects with integer keys. 
                          They are not suitable for sorting arbitrary data types or complex objects with custom comparison functions, 
                          which Merge Sort and Quick Sort can handle more efficiently.

3- Range of Values: Count Sort and Radix Sort perform well when the range of input values is relatively small compared to the number of elements. 
                    If the range is significantly larger, they may require excessive memory or time, 
                    making Merge Sort and Quick Sort more practical choices.


